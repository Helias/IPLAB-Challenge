{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, base_path, txt_list, transform=None):\n",
    "        self.base_path=base_path\n",
    "        self.images = np.loadtxt(txt_list,dtype=str,delimiter=',')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        f,_,_,_,_,c = self.images[index]\n",
    "\n",
    "        #carichiamo l'immagine tramite PIL\n",
    "        im = Image.open(path.join(self.base_path, f))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "\n",
    "        label = int(c)\n",
    "\n",
    "        return { 'image' : im, 'label':label, 'img_name': f }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms to calculate mean and standard_deviation\n",
    "#dataset = LocalDataset(\"images\", \"training_list.csv\", transform=transforms.ToTensor())\n",
    "\n",
    "# Mean\n",
    "#m = torch.zeros(3)\n",
    "#for sample in dataset:\n",
    "#    m += sample['image'].sum(1).sum(1)\n",
    "#m /= len(dataset)*256*144\n",
    "\n",
    "# Standard Deviation\n",
    "#s = torch.zeros(3)\n",
    "#for sample in dataset:\n",
    "#    s+=((sample['image']-m.view(3,1,1))**2).sum(1).sum(1)\n",
    "#s=torch.sqrt(s/(len(dataset)*256*144))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "\n",
    "# training\n",
    "EPOCHS = 1\n",
    "MOMENTUM = 0.9\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 32\n",
    "THREADS = 8\n",
    "\n",
    "# file paths\n",
    "IMAGES_PATH = \"images\"\n",
    "TRAINING_PATH = \"training_list.csv\"\n",
    "VALIDATION_PATH = \"validation_list.csv\"\n",
    "TEST_PATH = \"validation_list.csv\"\n",
    "\n",
    "RESULTS_PATH=\"results\"\n",
    "\n",
    "# regolarization\n",
    "WEIGHT_DECAY=0.005\n",
    "\n",
    "\n",
    "# directory results\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.makedirs(RESULTS_PATH)\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "# pre-computed mean and standard_deviation\n",
    "mean = torch.Tensor([0.3877, 0.3647, 0.3547])\n",
    "std_dev = torch.Tensor([0.2121, 0.2106, 0.2119])\n",
    "\n",
    "transform = transforms.Compose([transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std_dev)])\n",
    "\n",
    "training_set = LocalDataset(IMAGES_PATH, TRAINING_PATH, transform=transform)\n",
    "validation_set = LocalDataset(IMAGES_PATH, VALIDATION_PATH, transform=transform)\n",
    "test_set = LocalDataset(IMAGES_PATH, TEST_PATH, transform=transform)\n",
    "\n",
    "training_set_loader = DataLoader(dataset=training_set, batch_size=BATCH_SIZE, num_workers=THREADS, shuffle=True)\n",
    "validation_set_loader = DataLoader(dataset=validation_set, batch_size=BATCH_SIZE, num_workers=THREADS, shuffle=False)\n",
    "test_set_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, num_workers=THREADS, shuffle=False)\n",
    "\n",
    "classes = {\"num_classes\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, model, lr=LEARNING_RATE, epochs=EPOCHS, momentum=MOMENTUM, weight_decay=0, train_loader=training_set_loader, test_loader=test_set_loader):\n",
    "    \n",
    "    if not os.path.exists(RESULTS_PATH + \"/\" + model_name):\n",
    "        os.makedirs(RESULTS_PATH + \"/\" + model_name)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    loaders = {'train':train_loader, 'test':test_loader}\n",
    "    losses = {'train':[], 'test':[]}\n",
    "    accuracies = {'train':[], 'test':[]}\n",
    "\n",
    "    #testing variables\n",
    "    Y_testing = []\n",
    "    preds = []\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model=model.cuda()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for mode in ['train', 'test']:\n",
    "            if mode=='train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            samples = 0\n",
    "\n",
    "            for i, batch in enumerate(loaders[mode]):\n",
    "                \n",
    "                # convert tensor to variable\n",
    "                x=Variable(batch['image'], requires_grad=(mode=='train'))\n",
    "                y=Variable(batch['label'])\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "                output = model(x)\n",
    "                l = criterion(output, y) # loss\n",
    "                \n",
    "                if mode=='train':\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                else:\n",
    "                    Y_testing.extend(y.data.tolist())\n",
    "                    preds.extend(output.max(1)[1].tolist())\n",
    "                \n",
    "                acc = accuracy_score(y.data, output.max(1)[1]) # output.max(1)[1].data\n",
    "                \n",
    "                epoch_loss += l.data.item()*x.shape[0] # l.data[0]\n",
    "                epoch_acc += acc*x.shape[0]\n",
    "                samples += x.shape[0]\n",
    "\n",
    "                #print (\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\" % \\\n",
    "                #       (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss/samples, epoch_acc/samples))\n",
    "                \n",
    "                #debug\n",
    "                #if i == 2:\n",
    "                #    break\n",
    "\n",
    "            epoch_loss /= samples\n",
    "            epoch_acc /= samples\n",
    "            \n",
    "            losses[mode].append(epoch_loss)\n",
    "            accuracies[mode].append(epoch_acc)\n",
    "\n",
    "\n",
    "            print (\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\" % \\\n",
    "                  (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss, epoch_acc))\n",
    "            \n",
    "    torch.save(model.state_dict(), str(RESULTS_PATH) + \"/\" + str(model_name) + \"/\" + str(model_name) + \".pt\")\n",
    "    return model, (losses, accuracies), Y_testing, preds\n",
    "\n",
    "\n",
    "def test_model(model_name, model, test_loader = test_set_loader):\n",
    "    model.load_state_dict(torch.load(str(RESULTS_PATH) + \"/\" + str(model_name) + \"/\" + str(model_name) + \".pt\"))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    gts = []\n",
    "    \n",
    "    #debug\n",
    "    #i = 0\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        pred = model(Variable(batch['image'])).data.numpy().copy()\n",
    "        gt = batch['label'].numpy().copy()\n",
    "        preds.append(pred)\n",
    "        gts.append(gt)\n",
    "        \n",
    "        # debug\n",
    "        #if i == 2:\n",
    "        #    break\n",
    "        #i+=1\n",
    "        \n",
    "    return np.concatenate(preds), np.concatenate(gts)\n",
    "\n",
    "def write_stats(model_name, Y, predictions, gts, predictions2):\n",
    "    if not os.path.exists(RESULTS_PATH + \"/\" + model_name):\n",
    "        os.makedirs(RESULTS_PATH + \"/\" + model_name)\n",
    "\n",
    "    acc = accuracy_score(gts, predictions2.argmax(1))\n",
    "    cm = confusion_matrix(Y, predictions)\n",
    "    score = f1_score(Y, predictions, average=None)\n",
    "    # debug\n",
    "    #score = \"00 F1_SCORE 00\"\n",
    "    \n",
    "    file = open(str(RESULTS_PATH) + \"/\" + str(model_name) + \"/\" + str(model_name) + \"_stats.txt\", \"w+\")\n",
    "    file.write (\"Accuracy: \" + str(acc) + \"\\n\\n\")\n",
    "    file.write(\"Confusion Matrix: \\n\" + str(cm) + \"\\n\\n\")\n",
    "    file.write(\"F1 Score: \\n\" + str(score))\n",
    "    file.close()\n",
    "\n",
    "def plot_logs_classification(model_name, logs):\n",
    "    if not os.path.exists(RESULTS_PATH + \"/\" + model_name):\n",
    "        os.makedirs(RESULTS_PATH + \"/\" + model_name)\n",
    "\n",
    "    training_losses, training_accuracies, test_losses, test_accuracies = \\\n",
    "        logs[0]['train'], logs[1]['train'], logs[0]['test'], logs[1]['test']\n",
    "\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(training_losses)\n",
    "    plt.plot(test_losses)\n",
    "    plt.legend(['Training Loss','Test Losses'])\n",
    "    plt.grid()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(training_accuracies)\n",
    "    plt.plot(test_accuracies)\n",
    "    plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "    plt.grid()\n",
    "    #plt.show()\n",
    "    plt.savefig(str(RESULTS_PATH) + \"/\" + str(model_name) + \"/\" + str(model_name) + \"_graph.png\")\n",
    "\n",
    "def train_model_iter(model_name, model, weight_decay=0):\n",
    "    model, loss_acc, Y_testing, preds = train_model(model_name=model_name, model=model, weight_decay=weight_decay)\n",
    "    preds_test, gts = test_model(model_name, model=model)\n",
    "    write_stats(model_name, Y_testing, preds, gts, preds_test)\n",
    "    plot_logs_classification(model_name, loss_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet 18, 34, 50, 101, 152\n",
    "resnet18_model = resnet.resnet18(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet18\", resnet18_model)\n",
    "\n",
    "resnet34_model = resnet.resnet34(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet34\", resnet34_model)\n",
    "\n",
    "resnet50_model = resnet.resnet50(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet50\", resnet50_model)\n",
    "\n",
    "resnet101_model = resnet.resnet101(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet101\", resnet101_model)\n",
    "\n",
    "resnet152_model = resnet.resnet152(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet152\", resnet152_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization\n",
    "\n",
    "# Weight Decay\n",
    "resnet18_model = resnet.resnet18(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet18_wd\", resnet18_model, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "resnet34_model = resnet.resnet34(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet34_wd\", resnet34_model, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "resnet50_model = resnet.resnet50(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet50_wd\", resnet50_model, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "resnet101_model = resnet.resnet101(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet101_wd\", resnet101_model, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "resnet152_model = resnet.resnet152(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet152_wd\", resnet152_model, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "transform = transforms.Compose([transforms.RandomVerticalFlip(),\n",
    "                                transforms.ColorJitter(),\n",
    "                                transforms.RandomCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean, std_dev)])\n",
    "\n",
    "training_set = LocalDataset(IMAGES_PATH, TRAINING_PATH, transform=transform)\n",
    "validation_set = LocalDataset(IMAGES_PATH, VALIDATION_PATH, transform=transform)\n",
    "test_set = LocalDataset(IMAGES_PATH, TEST_PATH, transform=transform)\n",
    "\n",
    "training_set_loader = DataLoader(dataset=training_set, batch_size=BATCH_SIZE, num_workers=THREADS, shuffle=True)\n",
    "validation_set_loader = DataLoader(dataset=validation_set, batch_size=BATCH_SIZE, num_workers=THREADS, shuffle=False)\n",
    "test_set_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, num_workers=THREADS, shuffle=False)\n",
    "\n",
    "resnet18_model = resnet.resnet18(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet18_da\", resnet18_model)\n",
    "\n",
    "resnet34_model = resnet.resnet34(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet34_da\", resnet34_model)\n",
    "\n",
    "resnet50_model = resnet.resnet50(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet50_da\", resnet50_model)\n",
    "\n",
    "resnet101_model = resnet.resnet101(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet101_da\", resnet101_model)\n",
    "\n",
    "resnet152_model = resnet.resnet152(pretrained=False, **classes)\n",
    "train_model_iter(\"resnet152_da\", resnet152_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
